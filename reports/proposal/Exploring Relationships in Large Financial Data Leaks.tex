\documentclass[a4paper,latin]{paper} 
\usepackage[english]{babel}
\usepackage[textwidth=3cm,margin=1.2cm,columnsep=1cm,bottom=2cm, top=1.8cm]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{tcolorbox}
%\usepackage{tabular}

%\setlength\textwidth{\dimexpr (3in -1in/16)*2 + 3in/8\relax}
%\setlength\columnsep{\dimexpr 3in/8\relax}
%Import the natbib package and sets a bibliography  and citation styles
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear}
\usepackage{amsmath}
\usepackage{color}
\usepackage[linktocpage,colorlinks=true,linkcolor= {red!50!black}, urlcolor=grey, citecolor=blue!90, pdfborder={2 1 0}]{hyperref}

\usepackage{amsmath}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{refcount}
\usepackage{longtable}
%\usepackage{fancyvrb}
\usepackage{array}
\usepackage{tabularx}
 \usepackage{colortbl} 
\usepackage{subcaption}
\usepackage{caption}
\usepackage[numbib,nottoc]{tocbibind}

\usepackage[numbib,nottoc]{tocbibind}

\DeclareCaptionType{equ}[][]

\sectionfont{\large\sf\bfseries\color{black!70!blue}} 
%\renewcommand\keywordname{Clavem verborum}


\title{}
%\hfill\includegraphics[height=2cm]{/home/fran/logo}}
\author{Marcus Gawronsky}
% \author{Marcus Gawronsky \\ Supervisors: A/Prof. Tim Gebbie and A/Prof. Kanshukan Rajaratnam}
\institution{University of Cape Town \\ Masters in Advanced Analytics and Decision Sciences (STA5004W) Half-dissertation Research Proposal}

\begin{document} 
\twocolumn[\maketitle 
\hrule 
\vspace{-2mm}
\begin{abstract}

In this work we seek to model and visualise the relationship between the temporal and system states present in metadata from large public financial data breaches to various market events observed in publicly traded information, such as stock prices, stock market announcements and media statements.  In our approach, we aim to use unsupervised methods in state detection to find and identify reduced representations for this metadata and analyze its temporal evolution through time.  
  
\end{abstract}



\begin{keywords}
Luanda Leaks, Paradise Papers, Panama Papers, Natural Language Processing, Network Analysis
\end{keywords}
\hrule\bigskip
]


\section{Hypotheses} 
\begin{itemize}
\item Hypothesis: There exists a causal relationship between various meta-data components of large public financial data breaches and market events based the timelines of this information.
\item Secondary Hypothesis: The meta-data components of large public financial data breaches provide the necessary information in cross-correlating the entities found in these leaks with known bad entities.  

% We aim to contribute to fundamental research on combining and extracting with the large-scale heterogeneous data sets contained in these and other leaks. 

\end{itemize}

\section{Literature Review}
In January of 2020, Africa witnessed a data leak of some 715,000 emails, charts, contracts, audits and accounts which revealed a long history of corruption, nepotism and embezzlement between Isabel dos Santos, Africa's richest women, and the Angolan Government and State-owned Enterprises \citep{Barr2020}.  This data breach dubbed the Luanda Leaks follows a series of similar breaches in recent years which have exposed large scale corruption around the world. Over the last decade, data from the Offshore Leaks (2013), Bahamas Leaks (2016), Panama Papers (2016), Paradise Papers (2017 & 2018) has provided the opportunity for teams of software engineers and journalist to index and report on the countless cases of corruption and fraud found in these petabyte-scale data dumps marking a significant step in digital journalism \citep{ICIJ2020}. 
\\
\\
While many of the previously leaked documents have been kept private for security, privacy and ethical reasons, the International Consortium of Investigative Journalists (ICIJ), who serve as custodians for the leaked data, do provide metadata extracted from these documents for use by the general public.  This metadata is provided to the public through both an online portal for search and visualization and a graph database which can be downloaded for use in further research.  This database contains information on the named-entities mentioned in the leaked documents, as well as tags describing the relationships between these entities.  While the ICIJ does not make explicit mention of how these entities are extracted for each of the data leaks, their software does point to several open-source tools designed for named-entity recognition and conference resolution, such as Standford's Core NLP Named Entity Recognizer and the Apache OpenNLP software \citep{InternationalConsortiumofInvestigativeJournalists, lafferty2001conditional}. 
\\
\\
Inside of academia, these data leaks have seen interest across disciplines from research in Information Systems, to research in Tax Law and Multidimensional Visualization \citep{Zhuhadar2019, Wiedemann2018}.  While these leaks present many interesting opportunities for researchers across disciplines; to date, much of this research has been limited by the scope and quality of the metadata provided by the ICIJ.  It is for this reason that many researchers have relied on the methods of Network Analysis to identify relationships and clusters in the data.  For \cite{Hajek2017}, this meta provided the opportunity to compare and develop methods for identifying bad entities using the popular Node2Vec and PageRank algorithms.  For \cite{ODonovan2019} doing research on financial markets, this involved extracting features from this network graph for use in a regression model used to estimate the \$174 billion loss in market capitalization for the 388 companies mentioned in the Panama Papers.  
\\
\\
While the approaches explored in the literature have presented an array of promising findings, there still exists much room for researchers in combining these datasets for further analysis.  In recent years, the applications of popular machine learning and deep learning concepts such as convolution and Any2Vec embedding have seen a major impact in Network Analysis \citep{Hamilton2017, Pal2016}.  These methods new salable approaches to out-of-core graph representation and prediction, with interesting application to the metadata provided to the ICIJ.    

% used in this analysis is by no means complete investigation of all possible methodologies, this paper does provide interesting motivation for similar such studies on other large public financial data breaches, such as the Offshore, Luanda and Bahamas Leaks.  

% In the field of Finance, this metadata has provided interesting features for researchers to estimate the impact of these leaks on market capitalization.  For the 388 companies mentioned in the Panama Papers, work by \cite{ODonovan2019} used a regression-based approach using features derived from the graph meatadata. While the approach used in this analysis is by no means complete investigation of all possible methodologies, this paper does provide interesting motivation for similar such studies on other large public financial data breaches, such as the Offshore, Luanda and Bahamas Leaks.  
\\
% They explore this effect under the following model:

% \begin{equation}
%     \textit{CAR}_{i} = \alpha + \beta \textit{PanamaPapersExposure}_{i} + \rho X_{i} + \epsilon_{i}
% \end{equation}

% where $\textit{CAR}_{i}$ denotes the cumulative abnormal return (CAR) of firm $i$ around a given event window,$\textit{PanamaPapersExposure}_{i}$ is an indicator variable indicating whether the firm was mentioned in the data leak, $X_{i}$ represents a vector of country and industry fixed effects which the model aims to control for, and $\epsilon_{i}$ is an error term in the model.
% \\
% Using this model, the authors explore a range of hypothesis around the effects and interaction of the Panama Papers on market events.  
% \\
\\
% A major challenge in dealing with these data leaks is in extracting and combining these large heterogeneous data sets for downstream data analysis and modelling.  Many of the documents contained in these leaks combine structured data, such as spreadsheets and financial reports, and unstructured data, such as contracts and analyst reports. For researchers, the ability to extract and combine this data is critical, but constrained by the need for privacy.  While the task of extracting this data is challenging much progress has been made in the fields of Natural Language Processing, Differential Privacy, Network Analysis and automated financial statement analysis which provide an interesting motivation for further inquiry.  
\\
\\
% While these libraries do pull from a broad range of techniques in Named Entity Recognizer, many breakthrough in the the field have unlocked new opportunities for natural language understanding and document representation.
% \\
% \\
% The field of Natural Language Processing traces a rich history of research which follows a number of important topics in search, question-and-answering, translation and language generation.  The major challenge presented across these domains is how best to represent natural language data. Natural language data is incredible sparse and often relies on long-term dependencies in text in order to disambiguite meaning.  In recent years, the breakthroughs in word-vector, sequence-to-sequence and transformer models have unlocked many applications within industry which have unlocked many research efforts in model distillation, compression and transfer learning \citep{mikolov2013efficient, peters2018deep, vaswani2017attention, jiao2019tinybert}. 
% \\
% \\
% These methods have greatly improved the accuracy of Named Entity and Conference Resolution tasks and have been accompanied by similar breakthroughs at the intersection of Differential Privacy and Natural Language Understanding \citep{Feyisetan2019, Hu2019}. These breakthroughs provide interesting motivation in revisiting many of the studies using the metadata extracted from these leaks as well as a renewed interest in the impact of this critical data.  
\\
\\
% Much of the work of automated analysis of Financial Statements has been focused historically on the exploration of pricing kernels and fraud detection. In \cite{Perols2011}, a rich review of and extensive study of the methods and financial data included in studies in detecting financial statement fraud is provided which demonstrates both the challenging in model and hyper-parameter optimization, as well as in the domain knowledge required in the development artefacts designed for fraud detection. While much of these studies mentioned in this data focus on many traditional metrics used by analysts exploring financial statements some researchers have a look to augment the structured data in financial reports with sentiments scores computed the notes and analysis accompanying these documents.  \cite{Hajek2017} use a common dictionary of sentiment scores to augment their data for downstream modelling.  While their methods to sentiment scoring do not yield improved results compared to the models trained exclusively on financial data- this remains an interesting approach to dealing with heterogeneous data within the automated financial statement fraud detection.  
 

\section{Aims and Objectives}  
\begin{itemize}
\item[1] Investigate and re-implement existing finding on the metadata of more recent leaks.  
\item[2] Implement and explore methods in identifying the community structure and structural equivalence present in the network metadata.  
\item[2] Explore methods in graph representation for downstream studies on market impact.  
\item[3] Identify opportunities for incorporating alternate data into the existing metadata and research.
\end{itemize}




\section{Data Requirements Specification}
While the metadata and public documents provided by the ICIJ remain rich and valuable data sources, many interesting research has been done through combining this data with other rich data sources:  
\begin{itemize}
\item In \cite{ODonovan2019}, researchers combined a list of named entities found in these leaks with macroeconomic, company financials and market data to estimate market impact. 
\item In \cite{Joaristi2018}, researchers combine the graph data provided by the ICIJ showing the connections between named entities with a number of Open-source Blacklists issued by the UN and government agencies to try inferring bad entities mentioned in these documents.
\end{itemize}
  
\section{Systems Requirements Specification}
\begin{itemize}
\item This research should require significant compute. Modern Transformer models like BERT for natural language comprehension are estimated to use between 12GB - 16GB of GRAM and benefit greatly from multi-GPU training for transfer learning applications.  
\item While many of the documents contained in the leaks remain private, the extracted metadata provided to the public still represents a large and complex dataset for use by reserachers which would require significant storage.  
\item One ambition of this research is to gain access to the source data. While the data from the Luanda Leaks remains relatively small, previous leaks have range in the petabytes of size. This data is not cheap to store or transfer and would require a significant investment in hardware.  
\item The Python 3.7 project will aim to utilize many of the gold-standard libraries in the ecosystem for Natural Language Processing including AllenNLP, Spacy and Gensim.  
\item This project will aim to make any library code publicly available and open-sourced through either the PyPI or Anaconda online software repositories, accompanied by example notebooks and documentation for users to experiment with and incorporate into further research efforts.  
\end{itemize}
All software and test-cases will be made available via GitHub to ensure that the research is reproducible and can be replicated using test-data, test-code and the described and derived theory. 

\section{Project Milestone Deliverables}
\begin{table}[H]
\begin{tcolorbox}[tabularx*={\arrayrulewidth0.6mm}{X||X|X},fonttitle=\bfseries\large,fontupper=\normalsize\sffamily,
colback=white!10!white,colframe=black!40!,
coltitle=black,center title,toprule=1mm]
\centerline{\textbf{Date}} & \centerline{\textbf{Description}} & \centerline{\textbf{Deliverable}} \\\hline\hline
%\vspace{0.1mm}  \centerline{10 Jul 17} &  dGFKD  & Basic Materials \\
 %                     &   $\boldsymbol{\cdot}$ Introduce parfors    & 
%\\\hline
\vspace{0.1mm}  \centerline{28 Feb 2020} &  Proposed idea refined, finalized and accepted  & Final proposal document \\\hline
\vspace{0.1mm}  \centerline{1 May 2020} &  Initial Literature Review discussing findings in related work.   & Literature Review \\\hline
\vspace{0.1mm}  \centerline{31 May 2020} &  Initial techinical document proving the necessary algorithms and their outlines  & Technical Document \\\hline
\vspace{0.1mm}  \centerline{June 2020} & Data engineering discussion and technical documents  &  Data Engineering Proposal \\\hline
\vspace{0.1mm}  \centerline{July 2020} & Initial joint presentation to supervisors  &  Initial presentation \\\hline
\vspace{0.1mm}  \centerline{Aug 2020} & Initial EDA and data science scoping &  Exploratory Data Analysis \\\hline
\vspace{0.1mm}  \centerline{Sept 2020} & Finalization of Data Science and data engineering requirements and analysis.   &  Data Science and data engineering finalized \\\hline
\vspace{0.1mm}  \centerline{Oct 2020} & Final presentation to both supervisors   &  Supervisor Presentation \\\hline
\vspace{0.1mm}  \centerline{Nov 2020} & First draft copy for discussion and input &  First Draft \\\hline
\vspace{0.1mm}  \centerline{Dec 2020} & Final draft for review and discussion  &  Final Draft finalized \\\hline
\end{tcolorbox}
\caption{Key dates and deliverables for research project}
\end{table}
\bibliography{refs} % Entries are in the "refs.bib" file

% \appendix
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/scurve-mds.png}
%     \caption{Multidimensional Scaling applied to the S-curve Manifold Learning Problem}
%     \label{fig:scurve-mds}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/scurve-dimal.png}
%     \caption{DIMAL method from \cite{Pai2019a} applied to the S-curve Manifold Learning Problem}
%     \label{fig:scurve-dima}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/scurve-tsne.png}
%     \caption{t-SNE method applied to the S-curve Manifold Learning Problem}
%     \label{fig:scurve-tsne}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/scurve-ptsne.png}
%     \caption{A Siamese Neural Network approach to the Paramteric t-SNE method proposed by \cite{VanDerMaaten2009a} applied to the S-curve Manifold Learning Problem}
%     \label{fig:scurve-ptsne}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/scurve-lle.png}
%     \caption{Locally Linear Embedding (LLE) applied to the S-curve Manifold Learning Problem}
%     \label{fig:scurve-lle}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/scurve-plle.png}
%     \caption{A Siamese Neural Network approach to LLE applied to the S-curve Manifold Learning Problem}
%     \label{fig:scurve-plle}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/scurve-vae.png}
%     \caption{Variational Autoencoder (VAE) method proposed by \cite{kingma2013auto} applied to tbe S-curve Manifold Learning Problem}
%     \label{fig:scurve-vae}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/scurve-infovae.png}
%     \caption{InfoVAE method proposed by \cite{zhao2017infovae} applied to the S-curve Manifold Learning Problem}
%     \label{fig:scurve-infovae}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/nistdigits-mds.png}
%     \caption{Multidimensional Scaling applied to the NIST Digits dataset}
%     \label{fig:nistdigits-mds}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/nistdigits-dimal.png}
%     \caption{DIMAL method from \cite{Pai2019a} applied to the NIST Digits dataset}
%     \label{fig:nistdigits-dimal}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/nistdigits-tsne.png}
%     \caption{t-SNE method applied to the NIST Digits dataset}
%     \label{fig:nistdigits-tsne}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/nistdigits-ptsne.png}
%     \caption{A Siamese Neural Network approach to the Paramteric t-SNE method proposed by \cite{VanDerMaaten2009a} applied to NIST Digits dataset}
%     \label{fig:nistdigits-ptsne}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/nistdigits-lle.png}
%     \caption{Locally Linear Embedding (LLE) applied to NIST Digits dataset}
%     \label{fig:nistdigits-ptsne}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/nistdigits-plle.png}
%     \caption{A Siamese Neural Network approach to LLE applied to the NIST Digits dataset}
%     \label{fig:nistdigits-lle}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/nistdigits-vae.png}
%     \caption{Variational Autoencoder (VAE) method proposed by \cite{kingma2013auto} applied to NIST Digits dataset}
%     \label{fig:nistdigits-vae}
% \end{figure}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=9cm]{images/nistdigits-infovae.png}
%     \caption{InfoVAE method proposed by \cite{zhao2017infovae} applied to NIST Digits dataset}
%     \label{fig:nistdigits-infovae}
% \end{figure}
\end {document}



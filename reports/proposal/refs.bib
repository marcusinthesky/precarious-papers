
% Word2Vec
@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

% BERT
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

%% TinyBERT
@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}

%% DistilBERT
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

%% MobileBERT
@article{sunmobilebert,
  title={MOBILEBERT: TASK-AGNOSTIC COMPRESSION OF BERT FOR RESOURCE LIMITED DEVICES},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny}
}


% ELMO
@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

% Attention is all you need
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{brin1998anatomy,
  title={The anatomy of a large-scale hypertextual web search engine},
  author={Brin, Sergey and Page, Lawrence},
  year={1998}
}

%%FIN stat
@article{Perols2011,
abstract = {This study compares the performance of six popular statistical and machine learning models in detecting financial statement fraud under different assumptions of misclassification costs and ratios of fraud firms to nonfraud firms. The results show, somewhat surprisingly, that logistic regression and support vector machines perform well relative to an artificial neural network, bagging, C4.5, and stacking. The results also reveal some diversity in predictors used across the classification algorithms. Out of 42 predictors examined, only six are consistently selected and used by different classification algorithms: auditor turnover, total discretionary accruals, Big 4 auditor, accounts receivable, meeting or beating analyst forecasts, and unexpected employee productivity. These findings extend financial statement fraud research and can be used by practitioners and regulators to improve fraud risk models.},
author = {Perols, Johan},
doi = {10.2308/ajpt-50009},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/finstat.pdf:pdf},
issn = {02780380},
journal = {Auditing},
keywords = {Analytical auditing,Classification algorithms,Financial statement fraud,Fraud detection,Fraud predictors},
mendeley-groups = {2020/Proposals/Leaks},
number = {2},
pages = {19--50},
title = {{Financial statement fraud detection: An analysis of statistical and machine learning algorithms}},
volume = {30},
year = {2011}
}

@article{Hajek2017,
abstract = {Financial statement fraud has been serious concern for investors, audit firms, government regulators, and other capital market stakeholders. Intelligent financial statement fraud detection systems have therefore been developed to support decision-making of the stakeholders. Fraudulent misrepresentation of financial statements in managerial comments has been noticed in recent studies. As such, the purpose of this study was to examine whether an improved financial fraud detection system could be developed by combining specific features derived from financial information and managerial comments in corporate annual reports. To develop this system, we employed both intelligent feature selection and classification using a wide range of machine learning methods. We found that ensemble methods outperformed the remaining methods in terms of true positive rate (fraudulent firms correctly classified as fraudulent). In contrast, Bayesian belief networks (BBN) performed best on non-fraudulent firms (true negative rate). This finding is important because interpretable ``green flag” values (for which fraud is likely absent) could be derived, providing potential decision support to auditors during client selection or audit planning. We also observe that both financial statements and text in annual reports can be utilised to detect non-fraudulent firms. However, non-annual report data (analysts' forecasts of revenues and earnings) are necessary to detect fraudulent firms. This finding has important implications for selecting variables when developing early warning systems of financial statement fraud.},
author = {Hajek, Petr and Henriques, Roberto},
doi = {10.1016/j.knosys.2017.05.001},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/finstat2.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Annual reports,Feature selection,Financial statement fraud,Machine learning,Text mining},
mendeley-groups = {2020/Proposals/Leaks},
pages = {139--152},
publisher = {Elsevier B.V.},
title = {{Mining corporate annual reports for intelligent detection of financial statement fraud – A comparative study of machine learning methods}},
volume = {128},
year = {2017}
}


% Diff priv
@article{Feyisetan2019,
abstract = {Accurately learning from user data while providing quantifiable privacy guarantees provides an opportunity to build better ML models while maintaining user trust. This paper presents a formal approach to carrying out privacy preserving text perturbation using the notion of dx-privacy designed to achieve geo-indistinguishability in location data. Our approach applies carefully calibrated noise to vector representation of words in a high dimension space as defined by word embedding models. We present a privacy proof that satisfies dx-privacy where the privacy parameter epsilon provides guarantees with respect to a distance metric defined by the word embedding space. We demonstrate how epsilon can be selected by analyzing plausible deniability statistics backed up by large scale analysis on GloVe and fastText embeddings. We conduct privacy audit experiments against 2 baseline models and utility experiments on 3 datasets to demonstrate the tradeoff between privacy and utility for varying values of epsilon on different task types. Our results demonstrate practical utility ({\textless} 2{\%} utility loss for training binary classifiers) while providing better privacy guarantees than baseline models.},
archivePrefix = {arXiv},
arxivId = {1910.08902},
author = {Feyisetan, Oluwaseyi and Balle, Borja and Drake, Thomas and Diethe, Tom},
eprint = {1910.08902},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/3336191.3371856.pdf:pdf},
isbn = {9781450368223},
mendeley-groups = {2020/Proposals/Leaks},
pages = {178--186},
title = {{Privacy- and Utility-Preserving Textual Analysis via Calibrated Multivariate Perturbations}},
url = {http://arxiv.org/abs/1910.08902},
year = {2019}
}

@article{Hu2019,
abstract = {The goal of homomorphic encryption is to encrypt data such that another party can operate on it without being explicitly exposed to the content of the original data. We introduce an idea for a privacy-preserving transformation on natural language data, inspired by homomorphic encryption. Our primary tool is {\{}$\backslash$em obfuscation{\}}, relying on the properties of natural language. Specifically, a given text is obfuscated using a neural model that aims to preserve the syntactic relationships of the original sentence so that the obfuscated sentence can be parsed instead of the original one. The model works at the word level, and learns to obfuscate each word separately by changing it into a new word that has a similar syntactic role. The text encrypted by our model leads to better performance on three syntactic parsers (two dependency and one constituency parsers) in comparison to a strong random baseline. The substituted words have similar syntactic properties, but different semantic content, compared to the original words.},
archivePrefix = {arXiv},
arxivId = {1904.09585},
author = {Hu, Zhifeng and Havrylov, Serhii and Titov, Ivan and Cohen, Shay B.},
eprint = {1904.09585},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/1904.09585.pdf:pdf},
mendeley-groups = {2020/Proposals/Leaks},
title = {{Obfuscation for Privacy-preserving Syntactic Parsing}},
url = {http://arxiv.org/abs/1904.09585},
year = {2019}
}

%graph representation
@article{Pal2016,
abstract = {The analysis of social, communication and information networks for identifying patterns, evolutionary characteristics and anomalies is a key problem for the military, for instance in the Intelligence community. Current techniques do not have the ability to discern unusual features or patterns that are not a priori known. We investigate the use of deep learning for network analysis. Over the last few years, deep learning has had unprecedented success in areas such as image classification, speech recognition, etc. However, research on the use of deep learning to network or graph analysis is limited. We present three preliminary techniques that we have developed as part of the ARL Network Science CTA program: (a) unsupervised classification using a very highly trained image recognizer, namely Caffe; (b) supervised classification using a variant of convolutional neural networks on node features such as degree and assortativity; and (c) a framework called node2vec for learning representations of nodes in a network using a mapping to natural language processing.},
author = {Pal, Siddharth and Dong, Yuxiao and Thapa, Bishal and Chawla, Nitesh V. and Swami, Ananthram and Ramanathan, Ram},
doi = {10.1109/MILCOM.2016.7795391},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/pal2016deep.pdf:pdf},
isbn = {9781509037810},
journal = {Proceedings - IEEE Military Communications Conference MILCOM},
keywords = {Biological neural networks,Convolution,Erbium,Feature extraction,Machine learning,Measurement},
mendeley-groups = {2020/Proposals/Leaks},
pages = {588--593},
title = {{Deep learning for network analysis: Problems, approaches and challenges}},
year = {2016}
}

@article{Hamilton2017,
abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
archivePrefix = {arXiv},
arxivId = {1709.05584},
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
eprint = {1709.05584},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/1709.05584.pdf:pdf},
mendeley-groups = {2020/Proposals/Leaks},
pages = {1--24},
title = {{Representation Learning on Graphs: Methods and Applications}},
url = {http://arxiv.org/abs/1709.05584},
year = {2017}
}




%%%%%%%%%%%%%%%%%%%%%%%%
@misc{ICIJ2020,
author = {{International Consortium of Investigative Journalists}},
mendeley-groups = {2020/Proposals/Leaks},
title = {{About | ICIJ Offshore Leaks Database}},
url = {https://offshoreleaks.icij.org/pages/about},
urldate = {2020-01-29}
}


@misc{Barr2020,
author = {Barr, Caelainn},
booktitle = {The Gaurdian},
mendeley-groups = {2020/Proposals,2020/Proposals/Leaks},
title = {{Revealed: how Angolan ruler's daughter used her status to build {\$}2bn empire | World news | The Guardian}},
url = {https://www.theguardian.com/world/2020/jan/19/isabel-dos-santos-revealed-africa-richest-woman-2bn-empire-luanda-leaks-angola},
year = {2020}
}


@article{ODonovan2019,
abstract = {We exploit one of the largest data leaks, to date, to study whether and how firms use secret offshore vehicles. From the leaked data, we identify 338 listed firms as users of secret offshore vehicles and document that these vehicles are used to finance corruption, avoid taxes, and expropriate shareholders. Overall, the leak erased {\$}\backslash{\$}{\$}174 billion in market capitalization among implicated firms. Following the increased transparency brought about by the leak, implicated firms experience lower sales from perceptively corrupt countries and avoid less tax. We conservatively estimate that 1 in 7 firms have offshore secrets. Received May 29, 2017; editorial decision December 2, 2018 by Editor Itay Goldstein. Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
author = {O'Donovan, James and Wagner, Hannes F. and Zeume, Stefan},
doi = {10.1093/rfs/hhz017},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/SSRN-id2771095.pdf:pdf},
issn = {14657368},
journal = {Review of Financial Studies},
number = {11},
pages = {4117--4155},
title = {{The Value of Offshore Secrets: Evidence from the Panama Papers}},
volume = {32},
year = {2019}
}


@article{Zhuhadar2019,
abstract = {Exposing learners to cognitive computing concepts involves new learning strategies. Because cognitive computing is probabilistic, using massive sets of data is fundamental in understanding these concepts. One data set is the release of the Offshore Panama Papers Leaks Database (LeaksDB) in May 2016, in which researchers were able to access this graph database as part of the International Consortium of Investigative Journalists (ICIJ) Offshore Leaks investigation and to draw conclusions about companies, trusts, foundations, and funds incorporated in 21 tax havens. For the purpose of this research, GraphDB Server was installed and configured by faculty members from a mid-western university. In addition, DBPedia and GeoNames repositories were linked to LeaksDB, leading to the discovery of interesting patterns about these Geo facade relationships between Officers (persons or companies) and Countries.},
author = {Zhuhadar, Leyla and Ciampa, Mark},
doi = {10.1016/j.chb.2017.12.013},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/1-s2.0-S0747563217306933-main.pdf:pdf},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Cognitive computing,GraphDB,Learning analytics,Ontology,Panama papers},
pages = {507--518},
publisher = {Elsevier Ltd},
title = {{Leveraging learning innovations in cognitive computing with massive data sets: Using the offshore Panama papers leak to discover patterns}},
url = {https://doi.org/10.1016/j.chb.2017.12.013},
volume = {92},
year = {2019}
}
@article{Joaristi2018,
abstract = {The Panama Papers represent a large set of relationships between people, companies, and organizations that had affairs with the Panamanian offshore law firm Mossack Fonseca, often due to money laundering. In this paper, we address for the first time the problem of searching the Panama Papers for people and companies that may be involved in illegal acts. We use a collection of international blacklists of sanctioned people and organizations as ground truth for bad entities. We propose a new ranking algorithm, named Suspiciousness Rank Back and Forth (SRBF), that leverages this ground truth to assign a degree of suspiciousness to each entity in the Panama Papers. We experimentally show that our algorithm achieves an AUROC of 0.85 and an Area Under the Recall Curve of 0.87 and outperforms existing techniques.},
author = {Joaristi, Mikel and Serra, Edoardo and Spezzano, Francesca},
doi = {10.1109/ASONAM.2018.8508497},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/joaristi2018.pdf:pdf},
isbn = {9781538660515},
journal = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2018},
pages = {767--773},
publisher = {IEEE},
title = {{Inferring bad entities through the Panama papers network}},
year = {2018}
}

@article{lafferty2001conditional,
  title={Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
  author={Lafferty, John and McCallum, Andrew and Pereira, Fernando CN},
  year={2001}
}

%%%%%%%%%%%%% MENDELEY %%%%%%%%%%%%%%%%%%%%%%%%

@misc{InternationalConsortiumofInvestigativeJournalists,
author = {{Datashare}},
title = {{Analyze documents - Datashare}},
url = {https://icij.gitbook.io/datashare/all/analyze-documents},
urldate = {2020-01-29}
}

@article{Zhuhadar2019,
abstract = {Exposing learners to cognitive computing concepts involves new learning strategies. Because cognitive computing is probabilistic, using massive sets of data is fundamental in understanding these concepts. One data set is the release of the Offshore Panama Papers Leaks Database (LeaksDB) in May 2016, in which researchers were able to access this graph database as part of the International Consortium of Investigative Journalists (ICIJ) Offshore Leaks investigation and to draw conclusions about companies, trusts, foundations, and funds incorporated in 21 tax havens. For the purpose of this research, GraphDB Server was installed and configured by faculty members from a mid-western university. In addition, DBPedia and GeoNames repositories were linked to LeaksDB, leading to the discovery of interesting patterns about these Geo facade relationships between Officers (persons or companies) and Countries.},
author = {Zhuhadar, Leyla and Ciampa, Mark},
doi = {10.1016/j.chb.2017.12.013},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/1-s2.0-S0747563217306933-main.pdf:pdf},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Cognitive computing,GraphDB,Learning analytics,Ontology,Panama papers},
pages = {507--518},
publisher = {Elsevier Ltd},
title = {{Leveraging learning innovations in cognitive computing with massive data sets: Using the offshore Panama papers leak to discover patterns}},
url = {https://doi.org/10.1016/j.chb.2017.12.013},
volume = {92},
year = {2019}
}
@article{ODonovan2019,
abstract = {We exploit one of the largest data leaks, to date, to study whether and how firms use secret offshore vehicles. From the leaked data, we identify 338 listed firms as users of secret offshore vehicles and document that these vehicles are used to finance corruption, avoid taxes, and expropriate shareholders. Overall, the leak erased {\$}\backslash{\$}{\$}174 billion in market capitalization among implicated firms. Following the increased transparency brought about by the leak, implicated firms experience lower sales from perceptively corrupt countries and avoid less tax. We conservatively estimate that 1 in 7 firms have offshore secrets. Received May 29, 2017; editorial decision December 2, 2018 by Editor Itay Goldstein. Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
author = {O'Donovan, James and Wagner, Hannes F. and Zeume, Stefan},
doi = {10.1093/rfs/hhz017},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/SSRN-id2771095.pdf:pdf},
issn = {14657368},
journal = {Review of Financial Studies},
number = {11},
pages = {4117--4155},
title = {{The Value of Offshore Secrets: Evidence from the Panama Papers}},
volume = {32},
year = {2019}
}

@article{Joaristi2018,
abstract = {The Panama Papers represent a large set of relationships between people, companies, and organizations that had affairs with the Panamanian offshore law firm Mossack Fonseca, often due to money laundering. In this paper, we address for the first time the problem of searching the Panama Papers for people and companies that may be involved in illegal acts. We use a collection of international blacklists of sanctioned people and organizations as ground truth for bad entities. We propose a new ranking algorithm, named Suspiciousness Rank Back and Forth (SRBF), that leverages this ground truth to assign a degree of suspiciousness to each entity in the Panama Papers. We experimentally show that our algorithm achieves an AUROC of 0.85 and an Area Under the Recall Curve of 0.87 and outperforms existing techniques.},
author = {Joaristi, Mikel and Serra, Edoardo and Spezzano, Francesca},
doi = {10.1109/ASONAM.2018.8508497},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/joaristi2018.pdf:pdf},
isbn = {9781538660515},
journal = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2018},
pages = {767--773},
publisher = {IEEE},
title = {{Inferring bad entities through the Panama papers network}},
year = {2018}
}


@article{Wiedemann2018,
abstract = {Investigative journalism in recent years is confronted with two major challenges: (1) vast amounts of unstructured data originating from large text collections such as leaks or answers to Freedom of Information requests, and (2) multi-lingual data due to intensified global cooperation and communication in politics, business and civil society. Faced with these challenges, journalists are increasingly cooperating in international networks. To support such collaborations, we present the new version of new/s/leak 2.0, our open-source software for content-based searching of leaks. It includes three novel main features: (1) automatic language detection and language-dependent information extraction for 40 languages, (2) entity and keyword visualization for efficient exploration, and (3) decentral deployment for analysis of confidential data from various formats. We illustrate the new analysis capabilities with an exemplary case study.},
archivePrefix = {arXiv},
arxivId = {1807.05151},
author = {Wiedemann, Gregor and Yimam, Seid Muhie and Biemann, Chris},
doi = {10.1007/978-3-030-01159-8_30},
eprint = {1807.05151},
file = {:home/marcussky/Documents/Proposal/LuandaLeaks/1807.05151.pdf:pdf},
isbn = {9783030011581},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data journalism,Information extraction,Investigative journalism,Keyterm extraction,Named entity recognition},
pages = {313--322},
title = {{New/s/leak 2.0 – Multilingual information extraction and visualization for investigative journalism}},
volume = {11186 LNCS},
year = {2018}
}

